{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H1XeNvB51vnt"
   },
   "source": [
    "# Training an LSTM network on the Penn Tree Bank (PTB) dataset - Part II: TensorFlow eager execution  \n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks were first proposed by Sepp Hochreiter and [Jürgen Schmidhuber][1] in 1997 for modeling sequence data. [Christopher Olah][2] has nicely illustrated how they work. The fifth course in the [deep learning specialization][3] on Coursera teaches recurrent neural networks (RNN), of which the LSTM is a variant, in detail, and explains many interesting applications. For a succinct summary of the mathematics of these models, see, for example, [Stanford cs231n lecture 10][4] or [Greff, et al. (2016)][5].\n",
    "\n",
    "This is Part II of a series of illustrative examples of training an LSTM network. In these examples, an LSTM network is trained on the Penn Tree Bank (PTB) dataset to replicate some previously published work. The PTB dataset is an English corpus available from Tomáš Mikolov's web [page][6], and used by many researchers in language modeling experiments. It contains 929K training words, 73K validation words, and 82K test words. It has 10K words in its vocabulary. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals used this dataset in their ICLR 2015 [paper][7] where they showed that the correct place to implement dropout regularization in an RNN is in the connections between layers and not between time steps. To demonstrate the effectiveness of their regularization strategy, they reported word-level perplexities on the PTB dataset with three different networks: a \"small\" non-regularized LSTM, a \"medium\" regularized LSTM, and a \"large\" regularized LSTM. It is their \"small\" non-regularized LSTM model that is replicated in these examples.  \n",
    "\n",
    "Part I of this series presents an object-oriented design of the non-regularized LSTM network implemented in pure [Python][8]/[NumPy][9]. Equations are coded up from scratch to carry out the computations without dependencies on extraneous frameworks or libraries. This is a minimalist implementation, partly inspired by Andrej Karpathy's [minimalist character-level language model][14]. The program runs on a CPU. \n",
    "\n",
    "Part II (i.e. this notebook) shows how the same model can be easily implemented using [TensorFlow][10], the open-source framework originally developed by researchers and engineers from the Google Brain team within Google’s AI organization. The model is programmed in TensorFlow's \"[eager execution][11]\" imperative programming environment that evaluates operations immediately without building dataflow graphs. This is akin to regular Python programming following Python control flow. The program is executed in [Colaboratory][12] with GPU acceleration.\n",
    "\n",
    "Part III demonstrates how the model can be implemented using TensorFlow's low-level programming model in which you first define the dataflow [graph][13] and then create a TensorFlow [session][13] to run parts of the graph. In a dataflow graph, the nodes (ops) represent units of computation, and the edges (tensors) represent the data consumed or produced by a computation. Calling most functions in the TensorFlow low-level API merely adds operations and tensors to the default graph, but does not perform the actual computation. Instead, you compose these functions until you have a tensor or operation that represents the overall computation, such as performing one step of gradient descent, and then pass that object to a TensorFlow session to run the computation. This model is different from the familiar imperative model, but is a common model for parallel computing. The program is executed in [Colaboratory][12] with GPU acceleration.\n",
    "\n",
    "It is shown that all these implementations yield results which agree with each other and with those in [Zaremba et al. (2015)][7].\n",
    "\n",
    "[1]: http://people.idsia.ch/~juergen/\n",
    "[2]: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "[3]: https://www.coursera.org/specializations/deep-learning\n",
    "[4]: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf\n",
    "[5]: https://arxiv.org/abs/1503.04069\n",
    "[6]: http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "[7]: https://arxiv.org/abs/1409.2329\n",
    "[8]: https://www.python.org/\n",
    "[9]: http://www.numpy.org/\n",
    "[10]: https://www.tensorflow.org/\n",
    "[11]: https://www.tensorflow.org/guide/eager\n",
    "[12]: https://colab.research.google.com/notebooks/welcome.ipynb\n",
    "[13]: https://www.tensorflow.org/guide/graphs\n",
    "[14]: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "[15]: https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb\n",
    "[16]: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/rnn_ptb\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing an LSTM network with TensorFlow \"eager execution\"\n",
    "The implementation is similar to [this][16] TensorFlow tutorial example, but differs in the following respects:\n",
    "1. This implementation depends on TensorFlow API that requires an Nvidia GPU to run. The tutorial example switches API depending on whether GPU is available or not, and so can run on either GPU or CPU. \n",
    "2. This implementation uses `tf.data.Dataset` for the input pipeline whereas the tutorial predates the Dataset API.\n",
    "\n",
    "There is only one class, PTBModel, which is responsible for:\n",
    "1. Configuring the network layers. The `__init__` method of the PTBModel class configures the network layers. \n",
    "2. The `train` method trains an epoch. The `evaluate` method evaluates an epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_Kgm9eYKaR_"
   },
   "outputs": [],
   "source": [
    "#MIT License - Copyright (c) 2018 tmatha\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tf.enable_eager_execution()#execute only once after starting kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4ctjFCxKaSL"
   },
   "outputs": [],
   "source": [
    "#MIT License - Copyright (c) 2018 tmatha\n",
    "\n",
    "class PTBModel(object):\n",
    "  \"\"\"\n",
    "  LSTM network for language modeling for training/evaluation/testing on the PTB\n",
    "  dataset. Model similar to the TensorFlow eager execution tutorial example at\n",
    "  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/\n",
    "  python/examples/rnn_ptb. \n",
    "  The main differences are:\n",
    "    \n",
    "  -The tutorial uses tf.contrib.cudnn_rnn.CudnnLSTM when running on Nvidia GPU, \n",
    "   and tf.nn.rnn_cell.BasicLSTMCell when GPU is not available. This model only  \n",
    "   uses tf.contrib.cudnn_rnn.CudnnLSTM and so Nvidia GPU is always required. \n",
    "  -This model uses tf.data.Dataset for the input pipe line where as the tutorial\n",
    "   predates the Dataset API.\n",
    "  \n",
    "  Ref: \n",
    "    Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals, \n",
    "    \"Recurrent Neural Network Regularization\", ICLR 2015\n",
    "    \n",
    "  Imports:\n",
    "    tf\n",
    "    tf.nn\n",
    "    tf.math\n",
    "    tf.contrib.cudnn_rnn.CudnnLSTM - requires GPU\n",
    "    tf.layers.Dense\n",
    "    tf.train.GradientDescentOptimizer\n",
    "    tf.contrib.eager\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, \n",
    "               vocab_size, \n",
    "               embedding_dim, \n",
    "               hidden_dim, \n",
    "               num_layers, \n",
    "               initializer, \n",
    "               dropout_ratio):\n",
    "    \"\"\"Initalizes a model instance. Configures network layers.\n",
    "    \n",
    "    Args:\n",
    "      vocab_size: int;\n",
    "      embedding_dim: int; \n",
    "      hidden_dim: int; hidden state size, cell state size\n",
    "      num_layers: int; number of layers\n",
    "      initializer: initializer object for initializing kernels and biases\n",
    "      dropout_ratio: float; drop out ratio\n",
    "\n",
    "    Returns:\n",
    "      a model instance\n",
    "\n",
    "    Raises:\n",
    "      \n",
    "    \"\"\"\n",
    "    self._embedding=tf.get_variable(\n",
    "        'embedding',\n",
    "        shape=(vocab_size, embedding_dim),\n",
    "        initializer=initializer,\n",
    "        trainable=True)\n",
    "    \n",
    "    #crashes when input_mode='skip_input' or 'auto_select'\n",
    "    self._rnn=tf.contrib.cudnn_rnn.CudnnLSTM(\n",
    "        num_layers=num_layers, \n",
    "        num_units=hidden_dim, \n",
    "        dropout=dropout_ratio,\n",
    "        kernel_initializer=initializer,\n",
    "        input_mode='linear_input')\n",
    " \n",
    "    self._dense=tf.layers.Dense(\n",
    "        units=vocab_size,        \n",
    "        kernel_initializer=initializer)\n",
    "        \n",
    "    self._training=False\n",
    "    self._state=None\n",
    "    \n",
    "  def _call(self,inp):\n",
    "    \"\"\"Executes a forward prop through the network.\n",
    "    \n",
    "    Args:\n",
    "      inp: input (features) tf.int64 2-d tensor of shape (seq_len,batch_size)\n",
    "      \n",
    "    Returns:\n",
    "      y: output logits tf.float32 3-d tensor of shape (seq_len,batch_size,\n",
    "        vocab_size)\n",
    "        \n",
    "    Raises:\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    y=tf.nn.embedding_lookup(self._embedding, inp)     \n",
    "    y,self._state=self._rnn(y,initial_state=self._state,training=self._training)\n",
    "    y=self._dense(y)\n",
    "    \n",
    "    return y\n",
    "  \n",
    "  def _loss(self, inp, target):\n",
    "    \"\"\"Calls forward prop, gets logits, computes softmax, and calculates total\n",
    "    cross-entropy loss for a mini batch.\n",
    "    \n",
    "    Args:\n",
    "      inp: input (features) tf.int64 2-d tensor of shape (seq_len,batch_size)\n",
    "      target: target (labels) tf.int64 2-d tensor of shape (seq_len,batch_size)\n",
    "      \n",
    "    Returns:\n",
    "      loss: tf.float32 0-d tensor\n",
    "        \n",
    "    Raises:\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    y=self._call(inp)\n",
    "    return tf.math.reduce_sum(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=target,logits=y))\n",
    "  \n",
    "  def train(self, iterator, clip_norm, optimizer):\n",
    "    \"\"\"Runs an epoch of training.\n",
    "        \n",
    "    Args:\n",
    "      iterator: iterator object that yields (input,target) tuples where input  \n",
    "        and target both are tf.int64 2-d tensors of shape (seq_len,batch_size)\n",
    "      clip_norm: float; maximum global norm for clipping gradients\n",
    "      optimizer: optimizer object for applying gradients\n",
    "      \n",
    "    Returns:\n",
    "      A python list of losses, one loss (float) per step\n",
    "      \n",
    "    Raises:\n",
    "    \n",
    "    \"\"\"\n",
    "    self._training=True\n",
    "    self._state=None\n",
    "    losses=[]\n",
    "    \n",
    "    while True:\n",
    "      try:\n",
    "        inp,target=iterator.get_next()\n",
    "        loss,grad_var=tf.contrib.eager.implicit_value_and_gradients(self._loss)(\n",
    "            inp,target)\n",
    "        losses.append(loss.numpy())\n",
    "        \n",
    "        gradients,variables=zip(*grad_var)\n",
    "        clipped, global_norm=tf.clip_by_global_norm(gradients, clip_norm)\n",
    "        grad_var=zip(clipped, variables)\n",
    "        optimizer.apply_gradients(grad_var)\n",
    "      except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        \n",
    "    return losses\n",
    "        \n",
    "  def evaluate(self, iterator):\n",
    "    \"\"\"Runs an epoch of evaluation.\n",
    "        \n",
    "    Args:\n",
    "      iterator: iterator object that yields (input,target) tuples where input \n",
    "        and target both are tf.int64 2-d tensors of shape (seq_len,batch_size)\n",
    "      \n",
    "    Returns:\n",
    "      A python list of losses, one loss (float) per step\n",
    "      \n",
    "    Raises:\n",
    "    \n",
    "    \"\"\"\n",
    "    self._training=False\n",
    "    self._state=None\n",
    "    losses=[]\n",
    "    \n",
    "    while True:\n",
    "      try:\n",
    "        inp,target=iterator.get_next()       \n",
    "        losses.append(self._loss(inp,target).numpy())\n",
    "      except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        \n",
    "    return losses\n",
    "\n",
    "  @classmethod\n",
    "  def instance(cls, model_type='small', vocab_size=10000):\n",
    "    \"\"\"Returns a model instance.\n",
    "    \n",
    "    Args:\n",
    "      model_type: string; 'small', 'medium' or 'large'; only 'small' is\n",
    "        implemented presently\n",
    "      vocab_size: int;\n",
    "\n",
    "    Returns:\n",
    "      a model instance\n",
    "\n",
    "    Raises:\n",
    "      \n",
    "    \"\"\"\n",
    "    return {\n",
    "        'small':PTBModel(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=200,\n",
    "            hidden_dim=200,\n",
    "            num_layers=2,\n",
    "            initializer=tf.random_uniform_initializer(minval=-0.1,maxval=0.1),\n",
    "            dropout_ratio=0.),\n",
    "        'medium':None,\n",
    "        'large':None\n",
    "        }.get(model_type,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycbcLmwm1vn1"
   },
   "source": [
    "---\n",
    "\n",
    "## Data shaping\n",
    "In language modeling, both input (feature) and target (label) sequences are formed from the same original sequence. The target sequence is simply the input sequence itself advanced by one time step. Both input and target data have to be fed to the model in chunks or mini batches of the requisite shape. The requisite shape differes from API to API; some (e.g. `tf.keras.layers.CuDNNLSTM`) require mini batches of shape (batch size, sequence length), whereas others (e.g. `tf.contrib.cudnn_rnn.CudnnLSTM`) call for mini batches of shape (sequence length, batch size). Here, sequence length is the length of the RNN layer or subsequence, and batch size is the number of subsequences processed together in one step of gradient descent. In other words, gradients are computed and applied for a full mini batch at each step of gradient descent, and it takes \"steps\" number of mini batches to cover an epoch. If the data doesn't divide cleanly into an integral number of mini batches, the last remaining data is dropped. The following function takes a 1-d data sequence and returns a tuple of input and target arrays to be fed to a Dataset which in turn has to feed correctly-shaped mini batches to the model. The original sequential order is preserved throughout except for having to split the original sequence into the requisite number of subsequences to be processed together. This entails in-memory data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfakrv38V39R"
   },
   "outputs": [],
   "source": [
    "#MIT License - Copyright (c) 2018 tmatha\n",
    "\n",
    "def features_labels(data_array,batch_size,seq_len,batch_first=True):\n",
    "  \"\"\"Splits the sequential data into batch_size number of sub_sequences and \n",
    "  folds them into the requisite shape. This procedure is applied to the data to \n",
    "  derive the features array. This procedure is repeated to derive the labels \n",
    "  array also, except in this case the data is shifted by one time step. \n",
    "  Returns a named tuple of features and labels.\n",
    "  \n",
    "  Args:\n",
    "    data_array: np.int64 1-d numpy array of shape (size,)\n",
    "    batch_size: int;\n",
    "    seq_len: int; length of the rnn layer\n",
    "    batch_first: boolean; the returned numpy arrays will be of shape \n",
    "      (batch_size*steps, seq_len) if True and \n",
    "      (seq_len*steps, batch_size) if False\n",
    "      \n",
    "  Returns:\n",
    "    named tuple of features and labels, features and labels are np.int64 2-d \n",
    "      numpy arrays of shape (batch_size*steps, seq_len) if batch_first is True \n",
    "      and (seq_len*steps, batch_size) if batch_first is False\n",
    "    steps: int; number of mini batches in an epoch\n",
    "      \n",
    "  Raises:\n",
    "    ValueError: If input data_array is not 1-d\n",
    "\n",
    "  \"\"\"\n",
    "  if len(data_array.shape) != 1:\n",
    "    raise ValueError('Expected 1-d data array, '\n",
    "                     'instead data array shape is {} '.format(data_array.shape))\n",
    "  \n",
    "  def fold(used_array):\n",
    "    shaped_array=np.reshape(used_array,(batch_size,seq_len*steps),order='C')\n",
    "    \n",
    "    if batch_first:\n",
    "      return np.concatenate(np.split(shaped_array,steps,axis=1),axis=0)\n",
    "    else:\n",
    "      return np.transpose(shaped_array)\n",
    "\n",
    "  steps=(data_array.shape[0]-1)//(batch_size*seq_len)\n",
    "  used=batch_size*seq_len*steps\n",
    "  \n",
    "  features=fold(data_array[:used])\n",
    "  labels=fold(data_array[1:used+1])\n",
    "  \n",
    "  Data=collections.namedtuple('Data',['features','labels'])\n",
    "  return Data(features=features,labels=labels),steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdqfMObL1vn4"
   },
   "source": [
    "---\n",
    "\n",
    "## Training and evaluation\n",
    "The non-regularized LSTM network, as in [Zaremba et al. (2015)][7], is configured and trained on the [Penn Tree Bank dataset][6]. Training the network for 13 epochs in [Colaboratory][12] with GPU acceleration takes less than 9 minutes. The trained network is shown to replicate word-level perplexities previously reported.\n",
    "\n",
    "[6]: http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "[7]: https://arxiv.org/abs/1409.2329\n",
    "[12]: https://colab.research.google.com/notebooks/welcome.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8486,
     "status": "error",
     "timestamp": 1588363564774,
     "user": {
      "displayName": "Benjamin Spiegel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKwE9Ze1ppa_r4PcawJG0pkv4OwCvrYqITjgQrHog=s64",
      "userId": "13648635182098365987"
     },
     "user_tz": 420
    },
    "id": "5USjmkBUbE0J",
    "outputId": "f3446dc3-7e78-4971-a61e-513229491ba2"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c42684cc0793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TensorFlow vers. {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#*******************************************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#MIT License - Copyright (c) 2018 tmatha\n",
    "#*******************************************************************************\n",
    "# ---------------------- set up -----------------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "start_time=time.time()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info('date {}'.format(datetime.datetime.now()))\n",
    "logging.info('device {}'.format(tf.test.gpu_device_name()))\n",
    "logging.info('TensorFlow vers. {}'.format(tf.__version__))\n",
    "assert tf.test.is_gpu_available()\n",
    "\n",
    "#*******************************************************************************\n",
    "# ------------------ hyper parameters -----------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "batch_size=20\n",
    "seq_len=20\n",
    "clip_norm=5\n",
    "learning_rate=1.\n",
    "decay=0.5\n",
    "epochs=13\n",
    "epochs_no_decay=4\n",
    "\n",
    "#*******************************************************************************\n",
    "# ---------------- data: Python lists of strings ------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "with open('ptb.train.txt','r') as f1,open('ptb.valid.txt','r') as f2,open(\n",
    "    'ptb.test.txt','r') as f3:\n",
    "    seq_train=f1.read().replace('\\n','<eos>').split(' ')\n",
    "    seq_valid=f2.read().replace('\\n','<eos>').split(' ')\n",
    "    seq_test=f3.read().replace('\\n','<eos>').split(' ')\n",
    "\n",
    "seq_train=list(filter(None,seq_train))\n",
    "seq_valid=list(filter(None,seq_valid))\n",
    "seq_test=list(filter(None,seq_test))\n",
    "\n",
    "logging.info(seq_train[:10])\n",
    "logging.info(seq_valid[:10])\n",
    "logging.info(seq_test[:10])\n",
    "\n",
    "size_train=len(seq_train)\n",
    "size_valid=len(seq_valid)\n",
    "size_test=len(seq_test)\n",
    "logging.info('size_train {}, size_valid {}, size_test {}'.format(\n",
    "    size_train,size_valid,size_test))\n",
    "\n",
    "vocab_train=set(seq_train)\n",
    "vocab_valid=set(seq_valid)\n",
    "vocab_test=set(seq_test)\n",
    "\n",
    "assert vocab_valid.issubset(vocab_train)\n",
    "assert vocab_test.issubset(vocab_train)\n",
    "logging.info('vocab_train {}, vocab_valid {}, vocab_test {}'.format(\n",
    "    len(vocab_train),len(vocab_valid),len(vocab_test)))\n",
    "\n",
    "vocab_train=sorted(vocab_train)#must have deterministic ordering, so word2id \n",
    "                               #dictionary is reproducible across invocations\n",
    "word2id={w:i for i,w in enumerate(vocab_train)}\n",
    "id2word={i:w for i,w in enumerate(vocab_train)}\n",
    "\n",
    "#*******************************************************************************\n",
    "# -- data: np.int64 1-d numpy arrays -> np.int64 2-d numpy arrays of shape ----*\n",
    "# -- (seq_len*steps, batch_size) ----------------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "#Note tf.contrib.cudnn_rnn.CudnnLSTM requires input tensor to be of shape \n",
    "#(seq_len,batch_size,embedding_dim), where as tf.keras.layers.CuDNNLSTM \n",
    "#requires input tensor to be of shape (batch_size,seq_len,embedding_dim)\n",
    "ids_train=np.array([word2id[word] for word in seq_train],copy=False,order='C')\n",
    "ids_valid=np.array([word2id[word] for word in seq_valid],copy=False,order='C')\n",
    "ids_test=np.array([word2id[word] for word in seq_test],copy=False,order='C')\n",
    "\n",
    "data_train,steps_train=features_labels(\n",
    "    ids_train,batch_size,seq_len,batch_first=False)\n",
    "data_valid,steps_valid=features_labels(\n",
    "    ids_valid,batch_size,seq_len,batch_first=False)\n",
    "data_test,steps_test=features_labels(\n",
    "    ids_test,batch_size,seq_len,batch_first=False)\n",
    "\n",
    "#*******************************************************************************\n",
    "# ------------------------- datasets ------------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "dataset_train=tf.data.Dataset.from_tensor_slices(data_train).batch(seq_len,\n",
    "    drop_remainder=True)\n",
    "dataset_valid=tf.data.Dataset.from_tensor_slices(data_valid).batch(seq_len,\n",
    "    drop_remainder=True)\n",
    "dataset_test=tf.data.Dataset.from_tensor_slices(data_test).batch(seq_len,\n",
    "    drop_remainder=True)\n",
    "\n",
    "#*******************************************************************************\n",
    "# --------------------  model, optimizer --------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "model=PTBModel.instance(model_type='small',vocab_size=len(word2id))\n",
    "lr=tf.contrib.eager.Variable(initial_value=learning_rate, trainable=False)\n",
    "optimizer=tf.train.GradientDescentOptimizer(lr)\n",
    "\n",
    "#*******************************************************************************\n",
    "# ------------------------- initialize ----------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "perplexity_train=[]\n",
    "perplexity_valid=[]\n",
    "\n",
    "print('\\n'+' '*24+'TRAINING'+'\\n'+\n",
    "      'time'+' '*6+\n",
    "      'epochs'+' '*9+\n",
    "      'loss'+' '*15+\n",
    "      'perplexity'+'\\n'+\n",
    "      ' '*20+\n",
    "      'train'+' '*4+\n",
    "      'valid'+' '*7+\n",
    "      'train'+' '*7+\n",
    "      'valid'+'\\n'+\n",
    "      '======'+' '*4+\n",
    "      '======'+' '*4+\n",
    "      '====='+' '*4+\n",
    "      '====='+' '*7+\n",
    "      '====='+' '*7+\n",
    "      '=====')\n",
    "\n",
    "#*******************************************************************************\n",
    "# ---------------------- train and evaluate -----------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "for epoch in range(epochs):\n",
    "  iter_train=dataset_train.make_one_shot_iterator()\n",
    "  losses_train=model.train(iter_train,clip_norm,optimizer)\n",
    "  assert len(losses_train)==steps_train\n",
    "  loss_train_avg=sum(losses_train)/(len(losses_train)*seq_len*batch_size)\n",
    "  perplexity_train+=[(epoch+(step+1)/len(losses_train),math.exp(loss/(\n",
    "      seq_len*batch_size))) for step,loss in enumerate(losses_train)]\n",
    "  \n",
    "  iter_valid=dataset_valid.make_one_shot_iterator()\n",
    "  losses_valid=model.evaluate(iter_valid)\n",
    "  assert len(losses_valid)==steps_valid\n",
    "  loss_valid_avg=sum(losses_valid)/(len(losses_valid)*seq_len*batch_size)\n",
    "  perplexity_valid.append((epoch+1,math.exp(loss_valid_avg)))\n",
    "  \n",
    "  if epoch>epochs_no_decay-2:\n",
    "    lr.assign(lr*decay)\n",
    "\n",
    "  print('{:}'.format(datetime.timedelta(seconds=round(time.time()-start_time))),\n",
    "        '{:5.2f}'.format(epoch+1),\n",
    "        '{:5.2f}'.format(loss_train_avg),\n",
    "        '{:5.2f}'.format(loss_valid_avg),\n",
    "        '{:8.2f}'.format(math.exp(loss_train_avg)),\n",
    "        '{:8.2f}'.format(perplexity_valid[-1][1]),sep=' '*4)\n",
    "\n",
    "#*******************************************************************************\n",
    "# ----------------------- plot ------------------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "plt.plot([loss[0] for loss in perplexity_train],\n",
    "         [loss[1] for loss in perplexity_train],\n",
    "         linewidth=1,color='red',label='training')\n",
    "plt.plot([loss[0] for loss in perplexity_valid],\n",
    "         [loss[1] for loss in perplexity_valid],\n",
    "         linewidth=1,color='blue',label='validation', marker='o')\n",
    "plt.grid(True,which='both',axis='both')\n",
    "plt.title('Penn Treebank Corpus')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.axis([0,14,0,500])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#*******************************************************************************\n",
    "# ------------------------ test -----------------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "iter_test=dataset_test.make_one_shot_iterator()\n",
    "losses_test=model.evaluate(iter_test)\n",
    "assert len(losses_test)==steps_test\n",
    "loss_test_avg=sum(losses_test)/(len(losses_test)*seq_len*batch_size)\n",
    "print(\"\\nLoss/perplexity on test set after {} epochs: {:8.2f}/{:8.2f}\\n\".format(\n",
    "    epoch+1,loss_test_avg,math.exp(loss_test_avg)))\n",
    "\n",
    "#*******************************************************************************\n",
    "# ----------------------- finish ----------------------------------------------*\n",
    "#                                                                              *\n",
    "#*******************************************************************************\n",
    "logging.info(\"execution time - {}s\".format(\n",
    "    datetime.timedelta(seconds=round(time.time()-start_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhGiMMup1vn9"
   },
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "1. S. Hochreiter, and [J. Schmidhuber][1]. Long Short-Term Memory. Neural Computation, 9(8):1735-1780, 1997 \n",
    "\n",
    "2. Christopher Olah, [Understanding LSTM networks][2], colah's blog, 27 August 2015\n",
    "\n",
    "3. [Deep learning specialization][3], Taught by Andrew Ng, Kian Katanforoosh, and Younes Bensouda Mourri, Coursera \n",
    "\n",
    "4. Fei-Fei Li, Justin Johnson, and Serena Yeung, [Stanford cs231n lecture 10][4], 4 May 2017\n",
    "\n",
    "5. Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, and Jürgen Schmidhuber, \"[LSTM: A Search Space Odyssey][5]\", Transactions on Neural Networks and Learning Systems, 2016 *(Errata: In version 2 of the paper on arXiv, on page 2, the first equation under \"B. Backpropagation Through Time\" gives the derivative of the loss with respect to yt. In that equation, there should be an over bar over z, i, f and o, denoting gradients inside the non-linear activation functions.)*\n",
    "\n",
    "6. Andrej Karpathy, [Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy][14]\n",
    " \n",
    "7. Tomáš Mikolov's web page, [Penn Tree Bank (PTB) dataset][6]\n",
    "\n",
    "8. Wojciech Zaremba, IlyaSutskever, and Oriol Vinyals, \"[Recurrent Neural Network Regularization][7]\", ICLR 2015\n",
    "\n",
    "9. TensorFlow tutorial [example][16] with eager execution\n",
    "10. TensorFlow tutorial [example][15] with graph execution\n",
    "\n",
    "11. Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar, \"[On the convergence of Adam and beyond][17]\", ICLR 2018 *(Errata: On 'slide 3 Algorithms', 'slide 6 Primary cause for non-convergence', and 'slide 10 AMSGrad' of [Sashank's presentation at ICLR 2018][18], in three places the exponent of beta inside the square root should be t-j instead of t-i. In one place on slide 10 in the AMSGrad update equation, the exponent of beta inside the square root should be k-j instead of k-i. Also, note that 1<=k<=t is implied.)*\n",
    "\n",
    "[1]: http://people.idsia.ch/~juergen/\n",
    "[2]: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "[3]: https://www.coursera.org/specializations/deep-learning\n",
    "[4]: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf\n",
    "[5]: https://arxiv.org/abs/1503.04069\n",
    "[6]: http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "[7]: https://arxiv.org/abs/1409.2329\n",
    "[8]: https://www.python.org/\n",
    "[9]: http://www.numpy.org/\n",
    "[10]: https://www.tensorflow.org/\n",
    "[11]: https://www.tensorflow.org/guide/eager\n",
    "[12]: https://colab.research.google.com/notebooks/welcome.ipynb\n",
    "[13]: https://www.tensorflow.org/guide/graphs\n",
    "[14]: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "[15]: https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb\n",
    "[16]: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/rnn_ptb\n",
    "[17]: https://openreview.net/forum?id=ryQu7f-RZ\n",
    "[18]: https://www.facebook.com/iclr.cc/videos/2123421684353553/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_tfe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
